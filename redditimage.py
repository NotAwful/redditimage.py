'''
redditimages.py

by Devon Taylor
===

To-do:
1. Implement imgur() function
2. implement parameters/arguments for operation from command line

'''
import sys
import time
from time import sleep
import requests
import lxml
from lxml import etree

htmlparser = etree.HTMLParser()

# If verbose = True, show verbose information
verbose = True

def make_request(headers, url):
    '''
    :param headers: typically {'User-Agent': 'Mozilla/5.0'}
    :param url: the website to send a request to
    '''
    if not url.startswith("http://") and not url.startswith("https://"):
        url = "https://"+url
    if verbose: print("[i] Making request to %s." % url)
    resp = requests.get(url,headers)
    if verbose: print("[i] HTTP Status Code: %s" % resp.status_code)
    if resp.status_code == 429:
        print("[!] Rate limited. Exiting.")
        sys.exit()
    resp = etree.fromstring(resp.text, htmlparser)
    return resp

def ireddit(page):
    '''
    This function is used for collecting the images hosted by reddit in a
        particular subreddit.


    :param page: An object generated by make_request
    :return: a list object to be added to the pages set.
    '''
    local_tasks = []
    if verbose: print("About to enter i.redd.it check.")
    for link in page.xpath('//div[@data-domain="i.redd.it"]/@data-url'):
        if verbose: print("[i] Link found: %s." % link)
        local_tasks.append(link)
    if verbose and local_tasks:
        print("[i] i.redd.it list:")
        print(local_tasks)
    return local_tasks

def testparsers(testfile):
    if verbose: print("[i] Testting Parsers.")
    pages = set()
    subreddit = etree.fromstring(open(testfile).read(), htmlparser)
    pages.update(ireddit(subreddit))
    if verbose:
        print("[i] Contents of pages:")
        print(pages)
    for link in pages: print(link)

def reddit_scrape(subreddit, count):
    '''
    :param subreddit, type string: Which subreddit to query. Do not include /r/,
        just the subreddit name. Example: "Cyberpunk" rather than "/r/Cyberpunk"
    :param count, type int: the minimum number of image links to grab may return
        more images than requested. It grabs all the images on a page, then checks
        how many images it has gathered.
        [!] COUNT IS NOT IMPLEMENTED, ONLY SCANS THE FIRST PAGE
    :note: Adding new sources (like imgur), add another pages.update().
        The logic for checking whether or not a domain exists is in the functions.
    '''
    pages = set()
    headers = {'User-Agent': 'Mozilla/5.0'}
    current_page = make_request(headers, 'https://old.reddit.com/r/'+subreddit)
    #Add additional image host functions here.
    pages.update(ireddit(current_page))
    # Loop section for additional pages
    if (len(pages) < count) and verbose: print("[i] Checking next page.")
    while len(pages) < count:
        if verbose: print("[i] Number of links: %i" % len(pages))
        # this is hacky and there's a better solution
        for link in current_page.xpath('//span[@class="next-button"]/a/@href'):
            next_page = link
            if verbose: print("[i] Next page link: %s" % next_page)
            current_page = make_request(headers,next_page)
            #Add additional image host functions here.
            pages.update(ireddit(current_page))
        sleep(3)
    if verbose:
        print("[i] Contents of pages:")
        print(pages)
    for link in pages: print(link)

'''
Change to the name of the subreddit you are targeting and the number of
    images to grab.
'''
if __name__ == '__main__':
    # testparsers('cyberpunk.html')
    reddit_scrape("Cyberpunk", 10)
